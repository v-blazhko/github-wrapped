{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d062b69-bb59-4290-a995-20396e409d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a8faa-7930-447b-be93-a0e2d11f2eda",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1091cfcd-c9ab-402b-bd73-4f86b732eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your Github token here\n",
    "GITHUB_TOKEN = \"ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" # Your Github API token\n",
    "HEADERS = {\"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
    "           \"X-GitHub-Api-Version\": \"2022-11-28\"}\n",
    "# List your team members logins here\n",
    "TEAM_MEMBERS = [\"v-blazhko\"]  # Replace with your team's Github usernames (logins)\n",
    "ORG_NAME = \"github\"  # Replace with your organization name\n",
    "SINCE_DATE = \"2025-01-01T00:00:00Z\"  # Adjust for the past year/month/etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6ccf7a-630b-448d-8779-d7f70fddd396",
   "metadata": {},
   "source": [
    "## Functions to retrieve the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6ef29-1c98-4221-a124-41c345c1a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_org_repos(org_name):\n",
    "    \"\"\"Fetch all repositories for an organization.\"\"\"\n",
    "    url = f\"https://api.github.com/orgs/{org_name}/repos\"\n",
    "    params = {\"per_page\": 100}\n",
    "    repos = []\n",
    "    while url:\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        response.raise_for_status()\n",
    "        repos.extend(response.json())\n",
    "        url = response.links.get(\"next\", {}).get(\"url\")  # Handle pagination\n",
    "    return repos\n",
    "\n",
    "\n",
    "def fetch_comments(repo, endpoint):\n",
    "    \"\"\"Fetch comments for a given repository and endpoint.\"\"\"\n",
    "    url = f\"https://api.github.com/repos/{repo}/{endpoint}\"\n",
    "    params = {\"since\": SINCE_DATE, \"per_page\": 100}\n",
    "    comments = []\n",
    "    while url:\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        response.raise_for_status()\n",
    "        comments.extend(response.json())\n",
    "        url = response.links.get(\"next\", {}).get(\"url\")  # Handle pagination\n",
    "    return comments\n",
    "\n",
    "\n",
    "def fetch_comments_url(url):\n",
    "    \"\"\"Fetch comments or commits from a given url.\"\"\"\n",
    "    params = {\"since\": SINCE_DATE, \"per_page\": 100}\n",
    "    comments = []\n",
    "    while url:\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        response.raise_for_status()\n",
    "        comments.extend(response.json())\n",
    "        url = response.links.get(\"next\", {}).get(\"url\")  # Handle pagination\n",
    "    return comments\n",
    "\n",
    "\n",
    "def fetch_prs(repo):\n",
    "    \"\"\"Fetch pull requests for a repository.\"\"\"\n",
    "    url = f\"https://api.github.com/repos/{repo}/pulls\"\n",
    "    params = {\"state\": \"all\", \"since\": SINCE_DATE,\n",
    "              \"sort\": \"created\", \"direction\": \"desc\", \"per_page\": 100}\n",
    "    prs = []\n",
    "    while url:\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        response.raise_for_status()\n",
    "        response_prs = response.json()\n",
    "        prs.extend(response_prs)\n",
    "        if len(response_prs) > 0 and response_prs[-1][\"created_at\"] < SINCE_DATE:\n",
    "            break\n",
    "        url = response.links.get(\"next\", {}).get(\"url\")  # Handle pagination\n",
    "    return prs\n",
    "\n",
    "\n",
    "def filter_prs_by_date(prs):\n",
    "    \"\"\"Filter pull requests created after starting date.\"\"\"\n",
    "    return [pr for pr in prs if pr[\"created_at\"] >= SINCE_DATE]\n",
    "\n",
    "\n",
    "def filter_prs_by_collaboarators(prs):\n",
    "    \"\"\"Filter pull requests by team members.\"\"\"\n",
    "    return [pr for pr in prs if pr[\"user\"][\"login\"] in TEAM_MEMBERS]\n",
    "\n",
    "\n",
    "def get_commit_details(repo, commit_sha):\n",
    "    \"\"\"Fetch commit details.\"\"\"\n",
    "    commit_url = f\"https://api.github.com/repos/{repo}/commits/{commit_sha}\"\n",
    "    response = requests.get(commit_url, headers=HEADERS)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_gh_repo_commit_stats(repo):\n",
    "    \"\"\"Returns the last year of commit activity grouped by week.\n",
    "       This implementation fixes a bug with Github API returning empty list for data that has not been cached yet.\"\"\"\n",
    "    stats_url_ui = f\"https://github.com/{repo}/graphs/code-frequency\"\n",
    "    stats_url_api = f\"https://api.github.com/repos/{repo}/stats/code_frequency\"\n",
    "    _ = requests.get(stats_url_ui, headers=HEADERS)\n",
    "    time.sleep(1)\n",
    "\n",
    "    stats = {}\n",
    "    while len(stats) == 0:\n",
    "        response = requests.get(stats_url_api, headers=HEADERS)\n",
    "        stats = response.json()\n",
    "        time.sleep(1)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af6faf-d4f1-477f-a261-cf07b4bdb961",
   "metadata": {},
   "source": [
    "## Fetching organization repositories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e3f7b-f9ae-4f33-aa5e-8909fd6d05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fetching organization repositories...\")\n",
    "org_repos = fetch_org_repos(ORG_NAME)\n",
    "print(f\"Found {len(org_repos)} repositories in the organization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1dc81-6229-473f-bf65-84abd8213e7a",
   "metadata": {},
   "source": [
    "## Building the datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f1c4c4-44fe-4b9b-b8f4-2de50cadd959",
   "metadata": {},
   "source": [
    "Note: this implementation is aimed at medium-sized org and keeps the initial data structures in-memory. For larger data amounts, consider writing the datasets to disk, and creating the dataframes from those files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab84dc-63c8-4bf8-b9f8-b16c3e65b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Identifying repositories with contributions from team members...\")\n",
    "relevant_repos = []\n",
    "relevant_prs = []\n",
    "relevant_prs_comments = []\n",
    "relevant_prs_commits = []\n",
    "commits_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52259db8-63c6-4ab9-9ad3-e6b3adc9d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tqdm(org_repos, leave=True,  bar_format=\"{desc} {n_fmt}/{total_fmt}\")\n",
    "\n",
    "for repo in t:\n",
    "    t.set_description(\"%s\" % repo[\"full_name\"])\n",
    "    t.refresh()\n",
    "    try:\n",
    "        prs = fetch_prs(repo[\"full_name\"])\n",
    "        # Filter out pull requests by SINCE_DATE\n",
    "        # comment out to the get the full repo history\n",
    "        prs = filter_prs_by_date(prs)\n",
    "\n",
    "        # Filter out by collaborators\n",
    "        # comment out to the get PRs from all collaborators\n",
    "        prs = filter_prs_by_collaboarators(prs)\n",
    "\n",
    "        # Fetch the comments if there are any\n",
    "        for pr in prs:\n",
    "            pull_comments = fetch_comments_url(pr[\"review_comments_url\"])\n",
    "            issue_comments = fetch_comments_url(pr[\"comments_url\"])\n",
    "            commits = [commit[\"commit\"]\n",
    "                       for commit in fetch_comments_url(pr[\"commits_url\"])]\n",
    "            relevant_prs_comments.extend(pull_comments)\n",
    "            relevant_prs_comments.extend(issue_comments)\n",
    "            relevant_prs_commits.extend(commits)\n",
    "        if len(prs) > 0:\n",
    "            time.sleep(30)\n",
    "            relevant_repos.append(repo)\n",
    "            relevant_prs.extend(prs)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # Timeout in case of hitting Github API rate limit\n",
    "        # t.set_description(\"%s Exception, sleeping 1 minute\" % e)\n",
    "        # t.refresh()\n",
    "        # time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cadc7-52e4-47f0-b6b7-3fd9f341aff6",
   "metadata": {},
   "source": [
    "Due to the API design, this step is slow and can be skipped for optimization.\n",
    "Another way to pull the same stats is by using the Github Metadata API, but it has the limitation of only 10,000 commits per repository and will not work for bigger repos. See `get_gh_repo_commit_stats(repo)` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783cb2b-e672-4956-a5e0-e1cfb3dcbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for commit in tqdm(relevant_prs_commits):\n",
    "    repo = commit[\"url\"].split(\"/\")[4] + \"/\" + commit[\"url\"].split(\"/\")[5]\n",
    "    sha = commit[\"url\"].split(\"/\")[8]\n",
    "    stats = get_commit_details(repo, sha)[\"stats\"]\n",
    "    commits_stats.append({\"sha\": sha, \"additions\": stats[\"additions\"], \"deletions\": stats[\"deletions\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4278a-390a-4eed-b380-ef6717c7a55e",
   "metadata": {},
   "source": [
    "## Creating dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00847f35-385e-4c82-81bf-249de1590a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_df = pd.DataFrame.from_dict(relevant_repos)\n",
    "repos_df.to_csv(\"relevant_repos.csv\", index=False)  # saving for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302d2f7-6b9d-40fd-9209-9f6b1cfa2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df = pd.DataFrame.from_dict(relevant_prs)\n",
    "prs_df[\"user_login\"] = prs_df[\"user\"].apply(lambda d: d.get(\"login\"))\n",
    "prs_df[\"created_at\"] = pd.to_datetime(prs_df[\"created_at\"], utc=True)\n",
    "prs_df[\"repo_name\"] = prs_df[\"url\"].apply(lambda d: d.split(\"/\")[5])\n",
    "prs_df.to_csv(\"relevant_prs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74ea87-78e4-48e3-b18d-c6ffcca3a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.DataFrame.from_dict(relevant_prs_comments)\n",
    "comments_df[\"user_login\"] = comments_df[\"user\"].apply(lambda d: d.get(\"login\"))\n",
    "comments_df[\"repo_name\"] = comments_df[\"html_url\"].apply(lambda d: d.split(\"/\")[4])\n",
    "comments_df.to_csv(\"relevant_prs_comments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab5ccf-5af0-44ba-880f-c025f8fd15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_df = pd.DataFrame.from_dict(relevant_prs_commits)\n",
    "commits_df[\"user_login\"] = commits_df[\"committer\"].apply(lambda d: d.get(\"login\"))\n",
    "commits_df[\"repo_name\"] = commits_df[\"url\"].apply(lambda d: d.split(\"/\")[5])\n",
    "commits_df.to_csv(\"relevant_prs_commits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110f9e1-059b-4747-a34b-2842047790c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_stats_df = pd.DataFrame.from_dict(commits_stats)\n",
    "commits_stats_df.to_csv(\"commits_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0291413-e596-4b99-b25b-043cc0484eed",
   "metadata": {},
   "source": [
    "## Aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e71d80-6c96-4874-b4be-91d4e587b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total repos worked on\n",
    "total_repos = len(repos_df)\n",
    "# Total PRs open\n",
    "total_prs = len(prs_df)\n",
    "# New repos created\n",
    "new_repos_this_year = repos_df[repos_df[\"created_at\"] > SINCE_DATE][\"name\"].to_list()\n",
    "# Most active month\n",
    "prs_df[\"created_month_name\"] = prs_df[\"created_at\"].dt.month_name()\n",
    "top_month = prs_df[\"created_month_name\"].value_counts()\n",
    "top_month_t = (top_month.idxmax(), top_month.max())\n",
    "# Most active days\n",
    "prs_df[\"created_date\"] = prs_df[\"created_at\"].dt.date\n",
    "top_3_pr_dates = prs_df[\"created_date\"].value_counts().head(3)\n",
    "top_3_pr_dates_str = \", \".join([f\"{date}: {count}\" for date, count in top_3_pr_dates.items()])\n",
    "# Top PR openers\n",
    "top_5_pr_openers = prs_df[\"user_login\"].value_counts().head(5)\n",
    "top_5_pr_openers_str = \", \".join([f\"{user}: {count}\" for user, count in top_5_pr_openers.items()])\n",
    "# Top repos with most PRs opened\n",
    "top_3_repos = prs_df[\"repo_name\"].value_counts().head(3)\n",
    "top_3_repos_str = \", \".join([f\"{repo_name}: {count} PRs opened\" for repo_name, count in top_3_repos.items()])\n",
    "# Top 3 commenters\n",
    "top_3_commenters = comments_df[\"user_login\"].value_counts().head(3)\n",
    "top_3_commenters_str = \", \".join([f\"{user_login}: {count} comments\" for user_login, count in top_3_commenters.items()])\n",
    "# LGTM counts\n",
    "lgtm_counts = comments_df[comments_df[\"body\"].str.contains(\"lgtm\", case=False) | comments_df[\"body\"].str.contains(\"looks good\", case=False)]\n",
    "# Addidions and deletions\n",
    "total_additions = \"?\" if commits_stats_df.empty else commits_stats_df[\"additions\"].sum()\n",
    "total_deletions = \"?\" if commits_stats_df.empty else commits_stats_df[\"deletions\"].sum()\n",
    "\n",
    "print(f\"Total number of repos worked on this year: {total_repos} âœ…\")\n",
    "print(f\"Total PRs open this year: {total_prs} ğŸ’ª\")\n",
    "print(f\"Most active month - {top_month_t[0]} with {top_month_t[1]} PRs open ğŸ¯\")\n",
    "print(f\"{len(new_repos_this_year)} new repositories created - {\", \".join(new_repos_this_year)} ğŸ‰\")\n",
    "print(f\"Top 3 days with most PRs opened: {top_3_pr_dates_str} ğŸ“…\")\n",
    "print(f\"Top 5 PR openers: {top_5_pr_openers_str} ğŸš€\")\n",
    "print(f\"Most dynamic repositories: {top_3_repos_str} ğŸ“ˆ\")\n",
    "print(f\"{len(comments_df)} comments left! Top commenters: {top_3_commenters_str} ğŸ“¢\")\n",
    "print(f\"{len(lgtm_counts)} LGTMs given (ğŸ‘ğŸ»á´— _á´—)ğŸ‘ğŸ»\")\n",
    "print(f\"{len(commits_df)} commits created ğŸ”¥\")\n",
    "print(f\"Lines of code written: â• {total_additions}\")\n",
    "print(f\"Lines of code deleted: â– {total_deletions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
